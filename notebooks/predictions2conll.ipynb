{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the .conll files with the predictions from a model.\n",
    "\n",
    "This script uses the predictions (.json) obtained after executing the following code in the training notebook for the corresponding split (training, validation, or test):\n",
    "\n",
    "```python\n",
    "test_subset = tokenized_dataset[\"test\"].map(batched=True, batch_size=32, remove_columns=[\"id\",\"ner_tags\",\"tokens\"])\n",
    "test_subset = test_subset.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "test_df = test_subset.to_pandas()\n",
    "test_df.to_json(\"test_results.json\")\n",
    "```\n",
    "\n",
    "The idea is to align the predictions JSON (`test_results.json`) and the complete CoNLL file for the split (i.e. `test.conll`), so that we can use the same char offsets and tokens of the CoNLL and just substitute the ground truth label with the predicted one. \n",
    "\n",
    "Also, by having the offsets, we can split the predicted joint CoNLL into the different files (`e.g. caso-clinico-1.conll`) if we find that the next offset is lower than the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_JSON = \"../bsc-bio-ehr-es-meddoprof/best-2rvi973b/test_results.json\" # Path to JSON File containing the predictions resulting from training notebook\n",
    "HF_DATASET = \"../meddoprof-no-act-ner\" # Path to hugging face dataset (can be local or remote)\n",
    "MERGED_CONLL = \"../meddoprof-no-act-ner/test.conll\" # CoNLL file with all true labels for the split\n",
    "ORIGINAL_CONLLS_DIR = \"../meddoprof-no-act/test\" # Directory containing the CoNLL files (with true labels) of the split\n",
    "OUTPUT_DIR = \"../bsc-bio-ehr-es-meddoprof/best-2rvi973b/test_prediction_conlls\" # Do not include a trailing '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(PREDICTIONS_JSON)\n",
    "df['predicted_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(HF_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dataset[\"train\"].features[\"ner_tags\"].feature\n",
    "id2label = {idx: tag for idx, tag in enumerate(classes.names)}\n",
    "label2id = {tag: idx for idx, tag in enumerate(classes.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = df.apply(lambda x: x.apply(pd.Series).stack())\n",
    "# NaN comes from padding-added tokens. For ignored tokens (special characters and not-first subtokens of a word), label is -100\n",
    "df_tokens = df_tokens.dropna()\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "BASE_MODEL = \"../bsc-bio-ehr-es-drugtemist-es/best-5umrjpdk\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add label in string format (int to string)\n",
    "df_tokens['labels_str'] = df_tokens['labels'].apply(lambda x: 'IGN' if x not in id2label else id2label[x])\n",
    "df_tokens['predicted_label_str'] = df_tokens['predicted_label'].apply(lambda x: 'IGN' if x not in id2label else id2label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out predictions that should be ignored\n",
    "df_filtered = df_tokens[df_tokens['labels'] != -100.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference CoNLL (whole split)\n",
    "df_conll = pd.read_csv(MERGED_CONLL, sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
    "df_conll.columns = ['label', 'start', 'end', 'text']\n",
    "df_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct missing tokens due to truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_length'] = df['labels'].apply(len)\n",
    "too_long_level_0 = df[df['token_length'] >= 512].index\n",
    "df[df['token_length'] >= 512] # max input RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_too_long = tokenizer(dataset['test'][4349]['tokens'], is_split_into_words=True, return_length=True)\n",
    "len(dataset['test'][4349]['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset['test'][4349]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_filtered.reset_index()\n",
    "too_long_level_1 = df_flat[df_flat['level_0'].isin(too_long_level_0)].groupby('level_0')['level_1'].count().values\n",
    "too_long_level_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = df_filtered['labels_str'].to_list()\n",
    "predicted_labels_list = df_filtered['predicted_label_str'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level_0, level_1 in zip(too_long_level_0, too_long_level_1):\n",
    "    too_long_idx_flat = df_flat[(df_flat['level_0'] == level_0) & (df_flat['level_1'] == level_1)].index[0]\n",
    "    tokenized_too_long = tokenizer(dataset['test'][4349]['tokens'], is_split_into_words=True, return_length=True)\n",
    "    num_words = len(dataset['test'][level_0]['ner_tags'])\n",
    "    print(f\"{num_words - level_1 = }\")\n",
    "    for i in range(num_words - level_1):\n",
    "        labels_list.insert(too_long_idx_flat, 'O')\n",
    "        predicted_labels_list.insert(too_long_idx_flat, 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that both true labels from the dataset and CoNLL are the same\n",
    "assert labels_list ==  df_conll['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace true labels with predicted labels\n",
    "df_conll['label'] = predicted_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filenames of CoNLLs\n",
    "original_conlls = sorted([filename for filename in os.listdir(ORIGINAL_CONLLS_DIR) if filename.endswith('.conll')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the .conll files by using offset\n",
    "current_offset = 0\n",
    "file_idx = 0 # Position of file within all retrieved with listdir\n",
    "start_token_idx = 0 # Index within the dataframe that marks the start of a file\n",
    "for idx, line in df_conll.iterrows():\n",
    "    # If we reach the end of a file\n",
    "    if line['start'] < current_offset:\n",
    "        df_conll.loc[start_token_idx:idx-1].to_csv(OUTPUT_DIR + '/' + original_conlls[file_idx], sep='\\t', quoting=csv.QUOTE_NONE, header=None, index=False)\n",
    "        file_idx += 1\n",
    "        current_offset = 0\n",
    "        start_token_idx = idx\n",
    "    current_offset = line['end']\n",
    "# Add last document\n",
    "df_conll.loc[start_token_idx:idx].to_csv(OUTPUT_DIR + '/' + original_conlls[file_idx], sep='\\t', quoting=csv.QUOTE_NONE, header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
