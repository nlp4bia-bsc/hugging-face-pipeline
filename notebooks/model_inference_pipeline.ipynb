{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH=\"/home/jan/bsc/best-hlu3ln61\"\n",
    "TXT_PATH=\"/home/jan/bsc/cataccc\"\n",
    "OUTPUT_DIR=\"/home/jan/bsc/predictions_anns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "pipe = pipeline(\"token-classification\", MODEL_PATH, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_doc_idx = []\n",
    "samples = []\n",
    "for doc_idx, filename in enumerate([filename for filename in sorted(os.listdir(TXT_PATH)) if filename.endswith(\".conll\")]):\n",
    "#filename = \"/home/jan/bsc/cataccc/cc_ca39.conll\"\n",
    "    with open(os.path.join(TXT_PATH, filename), 'r') as file:\n",
    "        doc_sentences = []\n",
    "        sentence = []\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            if line == '\\n':\n",
    "                doc_sentences.append(' '.join(sentence))\n",
    "                sentence = []\n",
    "            else:\n",
    "                token = line.split('\\t')[3].strip()\n",
    "                sentence.append(token)\n",
    "            line = file.readline()\n",
    "        if sentence:\n",
    "            doc_sentences.append(' '.join(sentence))\n",
    "    sentence_doc_idx.extend([doc_idx]*len(doc_sentences))\n",
    "    samples.extend(doc_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OUTPUT_DIR) and len([ann_file for ann_file in os.listdir(OUTPUT_DIR) if ann_file.endswith('.ann')]) > 0:\n",
    "    raise Exception(\"Output directory already has annotations\")\n",
    "else:\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(' '.join([\"su\", \"metástasis\", '(', 'cáncer', 'de', 'pulmón', ')', ';', 'conllevó','a','COVID', '.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"su metástasis (cáncer de pulmón); conllevó a COVID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"su metástasis ( cáncer de pulmón ) ; conllevó a COVID .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[filename for filename in sorted(os.listdir(TXT_PATH)) if filename.endswith(\".conll\")][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_doc_idx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences_entities = pipe(samples[:10])\n",
    "sentences_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_entities in sentences_entities:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/jan/bsc/distemist-es/test/es-S1575-06202010000200004-1.txt\"\n",
    "with open(os.path.join(TXT_PATH, filename)) as file:\n",
    "    text = file.read()\n",
    "preds = pipe(text)\n",
    "if not preds:\n",
    "    open(os.path.join(OUTPUT_DIR, os.path.basename(filename).replace('.txt', '.ann')), 'w+').close()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(preds)\n",
    "# Remove leading whitespace for starting tokens\n",
    "df['word'] = df['word'].apply(lambda text: text[1:] if text[0] == \" \" else text)\n",
    "#df['word'] = df['word'].str.replace(r'^[\\s.,\\n]+|[\\s.,\\n]+$', '') # we would need to adjust the offsets (by doing leading and trailing separately)\n",
    "df['id'] = [f\"T{id+1}\" for id in df.index]\n",
    "df['label_offsets'] = df['entity_group'] + ' ' + df['start'].astype(str) + ' ' + df['end'].astype(str)\n",
    "df = df.drop(['entity_group', 'start', 'end', 'score'], axis=1)\n",
    "df = df[['id', 'label_offsets', 'word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, os.path.basename(filename).replace('.txt', '.ann')), quoting=csv.QUOTE_NONE, sep=\"\\t\", index=False, header=False)\n",
    "except Exception as e:\n",
    "    print(df['word'])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for filename in [filename for filename in os.listdir(TXT_PATH) if filename.endswith(\".txt\")]:\n",
    "    if os.path.exists(os.path.join(OUTPUT_DIR, os.path.basename(filename).replace('.txt', '.ann'))):\n",
    "        continue\n",
    "    with open(os.path.join(TXT_PATH, filename)) as file:\n",
    "        text = file.read()\n",
    "    preds = pipe(text)\n",
    "    if not preds:\n",
    "        open(os.path.join(OUTPUT_DIR, os.path.basename(filename).replace('.txt', '.ann')), 'w+').close()\n",
    "        continue\n",
    "    df = pd.DataFrame.from_records(preds)\n",
    "    # Remove leading whitespace for starting tokens\n",
    "    df['word'] = df['word'].apply(lambda text: text[1:] if text[0] == \" \" else text)\n",
    "    df['id'] = [f\"T{id+1}\" for id in df.index]\n",
    "    df['label_offsets'] = df['entity_group'] + ' ' + df['start'].astype(str) + ' ' + df['end'].astype(str)\n",
    "    df = df.drop(['entity_group', 'start', 'end', 'score'], axis=1)\n",
    "    df = df[['id', 'label_offsets', 'word']]\n",
    "    try:\n",
    "        df.to_csv(os.path.join(OUTPUT_DIR, os.path.basename(filename).replace('.txt', '.ann')), quoting=csv.QUOTE_NONE, sep=\"\\t\", index=False, header=False)\n",
    "    except Exception as e:\n",
    "        print(df['word'])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each document through pipe\n",
    "# Save result as dataframe\n",
    "# Dataframe to .ann\n",
    "# Without scores boolean\n",
    "# aggregation strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
